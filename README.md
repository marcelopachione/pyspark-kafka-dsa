# Lab Spark + Kafka
---

# Cluster Apache Spark com Docker

Este projeto fornece um ambiente completo para executar um **cluster Apache Spark** utilizando **Docker** e **Docker Compose**, facilitando a criaÃ§Ã£o, gerenciamento e testes locais com Spark. Ele Ã© ideal para desenvolvedores, analistas de dados e engenheiros que desejam experimentar o Spark em um ambiente distribuÃ­do sem a complexidade de configurar infraestrutura manualmente.

## âš™ï¸ Funcionalidades

- Cluster com 1 Spark Master e 3 Spark Workers
- Interface web do Spark Master acessÃ­vel em `http://localhost:9091`
- Spark History Server disponÃ­vel em `http://localhost:18081`
- Arquivo de exemplo para teste de submissÃ£o com PySpark
- Totalmente portÃ¡til e configurÃ¡vel via arquivos Docker

## ğŸš€ Tecnologias Utilizadas

- **Apache Spark 3.5.0**
- **Python 3**
- **PySpark**
- **Pandas**
- **Docker & Docker Compose**

## ğŸ‘¨â€ğŸ’» Objetivo

Este ambiente permite executar aplicaÃ§Ãµes Spark de forma local, simulando um cluster real. Ã‰ ideal para fins educacionais, testes de desempenho, desenvolvimento de pipelines de dados ou estudo de big data com PySpark.

## ğŸ“‚ Estrutura do Projeto

- `Dockerfile`: Cria a imagem base do Spark
- `docker-compose.yml`: Orquestra os serviÃ§os (master e workers)
- `build/conf/spark-defaults.conf`: ConfiguraÃ§Ãµes padrÃ£o do Spark
- `apps/`: DiretÃ³rio para adicionar scripts Spark em Python
- `.env.spark`: VariÃ¡veis de ambiente especÃ­ficas para o cluster

## ğŸ“„ Como Usar

Siga as instruÃ§Ãµes na seÃ§Ã£o [Spark_Docker](./build/readme.md) para clonar, configurar e iniciar o cluster Spark localmente usando Docker.

---